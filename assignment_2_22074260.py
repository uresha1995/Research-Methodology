# -*- coding: utf-8 -*-
"""newassign2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17nsYRkgPyTnmvloTDRCukJKFFXd4knUR
"""

!pip install --no-cache-dir transformers==4.38.2 datasets==2.18.0 evaluate==0.4.1

#Import libraries

from datasets import load_dataset
from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments
#from transformers import DataCollatorWithPadding
from torch.utils.data import DataLoader
#import evaluate
import torch
from tqdm import tqdm
import re
import random

#Download dataset

dataset = load_dataset("imdb")

#Print  random reviews

sample_texts = [dataset["train"][i]["text"] for i in random.sample(range(25000), 5)]

for i, text in enumerate(sample_texts):
    print(f"--- Sample {i+1} ---\n{text[:500]}\n")

"""Preprocessing"""

#Remove empty and short reviews

def rem_empty(example):
    return len(example["text"].strip()) > 20

#Remove HTML break tags and replace with space

def clean_text(example):
    text = example["text"]
    text = re.sub(r"<br\s*/?>", " ", text)
    example["text"] = text.strip()
    return example

dataset["train"] = dataset["train"].filter(rem_empty)
dataset["test"] = dataset["test"].filter(rem_empty)
dataset = dataset.map(clean_text)

"""Tokenization using BertTokenizerFast"""

#Making subset for fast the training

train_data = dataset["train"].shuffle(seed=42).select(range(2000))
test_data = dataset["test"].shuffle(seed=42).select(range(1000))

#Define tokenization function

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def tokenize_function(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",   # ensures equal-length batches
        max_length=128
    )

#Apply tokenization

#tokenized_train = small_train.map(tokenize_function, batched=True)
#tokenized_test = small_test.map(tokenize_function, batched=True)

#Convert to torch tensors

train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

from transformers import BertTokenizer, BertForSequenceClassification

#DataLoader

train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
test_loader = DataLoader(test_data, batch_size=16)

#Load model

#tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
#model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)

#Apply tokenization

#train_data = train_data.map(tokenize_function, batched=True)
#test_data = test_data.map(tokenize_function, batched=True)

#Set device

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#model.to(device)

#Define training arguments

#training_args = TrainingArguments(
#    output_dir="./results",
 #   evaluation_strategy="epoch",
  #  save_strategy="epoch",
   # logging_dir="./logs",
    #logging_steps=50,
    #learning_rate=2e-5,
    #per_device_train_batch_size=16,
    #per_device_eval_batch_size=16,
    #num_train_epochs=2,
    #weight_decay=0.01,
    #load_best_model_at_end=True,
    #report_to="none"
#)

#Convert tokenized dataset into torch tensors

#tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
#tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "label"])

#Prepare dataLoader

#train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True)
#test_loader = DataLoader(tokenized_test, batch_size=16)

#Move model device

#from transformers import BertForSequenceClassification

#model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#model.to(device)

#Optimizer

from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

from tqdm import tqdm

epochs = 2
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader):
        batch = {k: v.to(device) for k, v in batch.items()}
        batch['labels'] = batch.pop('label')  # Ensure label is correctly named
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1} - Training Loss: {total_loss:.4f}")

from sklearn.metrics import accuracy_score, classification_report

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluating", leave=False):
        batch = {k: v.to(device) for k, v in batch.items()}
        if 'label' in batch:
            batch['labels'] = batch.pop('label')
        outputs = model(**batch)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        all_preds.extend(predictions.cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())

acc = accuracy_score(all_labels, all_preds)

print(f"Test Accuracy: {acc:.4f}")
print(f"Classification Report:")
print(classification_report(all_labels, all_preds, target_names=["Negative", "Positive"]))
