{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7mLGusj1J92cItWXQ7Ksp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uresha1995/Research-Methodology/blob/main/Assignment_2_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Stk8VRdqod"
      },
      "outputs": [],
      "source": [
        "#Install the libraries\n",
        "\n",
        "!pip install --upgrade transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "import torch\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "JiI0QVXHdwWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "XizY2S9edyVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print  random reviews\n",
        "\n",
        "sample_texts = [dataset[\"train\"][i][\"text\"] for i in random.sample(range(25000), 5)]\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"--- Sample {i+1} ---\\n{text[:500]}\\n\")"
      ],
      "metadata": {
        "id": "-DS4f_B6d2hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "5UtIoSoYd5ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove empty and short reviews\n",
        "\n",
        "def rem_empty(example):\n",
        "    return len(example[\"text\"].strip()) > 20\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].filter(rem_empty)\n",
        "dataset[\"test\"] = dataset[\"test\"].filter(rem_empty)"
      ],
      "metadata": {
        "id": "b8FoIf_wd8_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing unnecessary spaces\n",
        "\n",
        "def clean_text(example):\n",
        "    example[\"text\"] = example[\"text\"].strip()\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(clean_text)"
      ],
      "metadata": {
        "id": "qrNmODypeAIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove HTML break tags and replace with space\n",
        "\n",
        "def clean_text(example):\n",
        "    text = example[\"text\"]\n",
        "    text = re.sub(r\"<br\\s*/?>\", \" \", text)\n",
        "    example[\"text\"] = text\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(clean_text)"
      ],
      "metadata": {
        "id": "94D_Ij7ceJqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_fun(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_fun, batched=True)"
      ],
      "metadata": {
        "id": "jZ-aTHiHeLWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for training"
      ],
      "metadata": {
        "id": "yOkqJu8PeSRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset into training and test\n",
        "\n",
        "train_data = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "test_data = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "E27j_QqTeWCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading BERT model\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "id": "MuOaEQ_weZP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers"
      ],
      "metadata": {
        "id": "zCtcC0EAeZ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \"transformers==4.41.0\" \"datasets==2.18.0\" \"evaluate==0.4.1\""
      ],
      "metadata": {
        "id": "2xd5OgaBefaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    #Save the model outputs\n",
        "    output_dir=\"./results\",\n",
        "\n",
        "    #Run evaluation of each epoch\n",
        "    evaluation_strategy=\"epoch\",\n",
        "\n",
        "    #Log training progress every few steps\n",
        "    logging_strategy=\"steps\",\n",
        "\n",
        "    #Logging to every 50 steps\n",
        "    logging_steps=50,\n",
        "\n",
        "    #Standard fine-tuning learning rate\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    #Tranining batch size\n",
        "    per_device_train_batch_size=16,\n",
        "\n",
        "    #Batch size for evaluation\n",
        "    per_device_eval_batch_size=16,\n",
        "\n",
        "    #Number of epochs\n",
        "    num_train_epochs=2,\n",
        "\n",
        "    # Weight decay for regularization\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    #where to store logs\n",
        "    logging_dir=\"./logs\",\n",
        "\n",
        "    #loading the best model\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    #Save the epoch end of the model\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "W4TNf2h6ehHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "p-ruxF-Eej3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}